
name: Tests

on:
  push:
    branches:
      - master
      - develop
  pull_request:
  release:
    types:
      - created


jobs:

  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # Developer Container:
  #
  #   - Since all tests run in a slightly different and reduced test container we must check the dev environment
  #     explicitly. The container must build, plus the website and test system must be working.
  #
  dev_container_checks:
    name: Dev. Container Checks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Build
        run: |
          cd docker
          docker-compose -f docker-compose.dev.yml up -d
          sleep 30

      - name: Check Symfony Application
        # would be nice to check the reset command in the future, in the current state the reset command can't fail
        run: |
          docker exec app.catroweb bin/console about
          docker exec app.catroweb bin/console catrobat:reset --hard

      - name: Check local share website
        run: |
          sudo apt-get install apache2 wget
          sudo sh -c "echo '\n127.0.0.1 catroweb' >> /etc/hosts"
          sudo service apache2 reload
          wget --spider -S "http://catroweb:8080" 2>&1 | awk '/HTTP\// {print $2}' | grep 200

      - name: Check tests in dev environment
        run:
          docker exec app.catroweb bin/behat -s web-general tests/behat/features/web/general/homepage.feature

      - name: Check shared development volumes must not need a rebuild
        id: shared-test-run-must-fail
        continue-on-error: true
        run: |
          echo ::set-output name=status::failure
          echo "INVALID" > tests/behat/features/web/general/homepage.feature
          docker exec app.catroweb bin/behat -s web-general tests/behat/features/web/general/homepage.feature
          echo ::set-output name=status::success

      - name: Check that invalid changes result in a failing testcase
        if: steps.shared-test-run-must-fail.outputs.status == 'success'
        run: |
          exit -1

  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # Build:
  #
  #   - In order to save computation time the "app.catroweb" image is only build once during this build phase.
  #     Other jobs can download this image to reduce their build time. With several jobs + the matrix build total
  #     computation time for this workflow can be highly reduced. This is important since we do not have unlimited
  #     resources/machines to run the jobs.
  #
  build:
    name: Build catroweb image
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Build catroweb app image
        run: |
          cd docker
          docker-compose -f docker-compose.test.yml build app.catroweb
          docker save app.catroweb > catroweb-image.tar

      - name: Upload app.catroweb image
        uses: actions/upload-artifact@v2
        with:
          name: catroweb-image
          path: docker/catroweb-image.tar

  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # PhpUnit:
  #
  #   - the tests are executed in our docker since we have many integration tests which access the database, etc.
  #     One might consider to strictly separate integration and unit tests. Units tests could be executed using
  #     composer scripts only to reduce the runtime to a few seconds. No build needed + dependencies can be easy cached.
  #
  #   - A code coverage report is pushed to the artifacts where it can be downloaded directly on GitHub.
  #     Keep in mind the report is not including the tests written for behat.
  #
  tests_phpunit:
    name: PhpUnit
    needs: build
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Pull the latest images to build (Faster than caching)
        run: |
          cd docker
          docker-compose -f docker-compose.test.yml pull

      - name: Download app.catroweb image
        uses: actions/download-artifact@v2
        with:
          name: catroweb-image
          path: docker

      - name: Build
        run: |
          cd docker
          docker load < catroweb-image.tar
          docker-compose -f docker-compose.test.yml up -d

      - name: PhpUnit tests
        run:
          docker exec app.catroweb bin/phpunit --coverage-html tests/testreports/coverage/phpunit --coverage-clover=tests/testreports/coverage/phpunit/coverage.xml

      - name: Upload code coverage report
        uses: actions/upload-artifact@v2
        if: always()
        with:
          name: PhpUnitTestReport
          path: tests/testreports/coverage/phpunit

      - uses: codecov/codecov-action@v1
        with:
          # token: ${{ secrets.CODECOV_TOKEN }} # not required for public repos
          file: tests/testreports/coverage/phpunit/coverage.xml
          flags: phpunit # optional
          name: codecov-umbrella # optional
          fail_ci_if_error: true # optional (default = false)


  # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  # Behat:
  #
  #  - This job runs all Behat suites parallel using a matrix strategy. This is done since integrations tests which
  #    are interacting with a browser and the GUI are extremely slow. With a total run-time of over an hour, using this
  #    approach the run-time can be drastically reduced. The disadvantage, we can't easily create a coverage report
  #    for the behat scenarios. Something that is considered to be a bad practice anyway since Behat is intended to
  #    deliver scenario automation in BDD.
  #
  #  - Behat and especially UI tests using Mink tend to flaky.
  #    A test will only be marked as failed if a test fails more than 3 times in a row.
  #    Flaky tests should be reduced to a minimum in the codebase!
  #
  #  - Behat only reruns failed tests - Not pending/missing tests or those with exceptions!
  #    A pending/missing test will NOT result in a failed pipeline!
  #    This is the reason why the explicit check for the log file had to be added.
  #
  #  - To ease the debugging, besides a log file, screenshots of failing tests are uploaded as artifacts.
  #
  #  Notes:
  #    - Check the behat.yml when changing / creating new suites
  #    - suites will finish their work even if another suite fails (fail-fast: false)
  #
  tests_behat:
    name: Behat
    needs: build
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        testSuite:
          - api
          - api-deprecated
          - web-admin
          - web-authentication
          - web-gamejam
          - web-general
          - web-notifications
          - web-profile
          - web-project-details
          - web-project-details-2
          - web-project-details-3
          - web-project-loader
          - web-search
          - web-top-bar

    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Pull the latest images to build (Faster than caching)
        run: |
          cd docker
          docker-compose -f docker-compose.test.yml pull

      - name: Download app.catroweb image
        uses: actions/download-artifact@v2
        with:
          name: catroweb-image
          path: docker

      - name: Build
        run: |
          cd docker
          docker load < catroweb-image.tar
          docker-compose -f docker-compose.test.yml up -d
          sleep 30

      # Test Run
      - name: Behat ${{ matrix.testSuite }} tests
        id: test-run
        continue-on-error: true
        # - The output will of the tests will be piped to the stdout and into the log file.
        # - A return code != 0 stops the execution of further commands in the pipeline.
        #   "tee" always returns 0, even if the behat test fails. Therefore we need to exit with the first entry of
        #   the pipe status, which contains the correct exit code.
        # - If the tests pass we can set the output status to success, this allows to only execute the reruns if needed
        run: |
          echo ::set-output name=status::failure
          docker exec app.catroweb bin/behat -s ${{ matrix.testSuite }} \
            | tee tests/testreports/behat/${{ matrix.testSuite }}.log \
            ; ( exit ${PIPESTATUS[0]} )
          echo ::set-output name=status::success

      # Missing steps are not rerun by behat, without this step they will be lost in the process
      # We must explicitly kill the pipeline if the log contains undefined steps
      - name: Check that suite has NO missing steps
        if: always()
        id: missing-check
        run: |
          if grep -q 'has missing steps. Define them with these snippets:' tests/testreports/behat/${{ matrix.testSuite }}.log; then
            cat tests/testreports/behat/${{ matrix.testSuite }}.log;
            false;
          fi

      # Pending steps are not rerun by behat, without this step they will be lost in the process
      # We must explicitly kill the pipeline if the log contains pending steps
      - name: Check that suite has NO pending steps
        if: always()
        id: pending-check
        run: |
          if grep -q 'pending)' tests/testreports/behat/${{ matrix.testSuite }}.log; then
            cat tests/testreports/behat/${{ matrix.testSuite }}.log;
            false;
          fi

      # Chrome exception are problems that can't be fixed with a rerun
      - name: Check that suite has NO chrome exceptions
        if: always()
        id: chrome-exception-check
        run: |
          if grep -q 'DMore\ChromeDriver\StreamReadException' tests/testreports/behat/${{ matrix.testSuite }}.log; then
            cat tests/testreports/behat/${{ matrix.testSuite }}.log;
            false;
          fi

      - uses: codecov/codecov-action@v1
        with:
          # token: ${{ secrets.CODECOV_TOKEN }} # not required for public repos
          file: tests/testreports/coverage/behat/coverage.xml
          flags: behat # optional
          name: codecov-umbrella # optional
          fail_ci_if_error: true # optional (default = false)

      # Rerun #1
      - name: 1. Rerun for Behat ${{ matrix.testSuite }} tests
        if: steps.test-run.outputs.status != 'success'
        id: test-rerun-1
        continue-on-error: true
        run: |
          echo ::set-output name=status::failure
          docker exec app.catroweb bin/behat -s ${{ matrix.testSuite }} --rerun
          echo ::set-output name=status::success

        # Rerun #2
      - name: 2. Rerun for Behat ${{ matrix.testSuite }} tests
        if: steps.test-run.outputs.status != 'success' && steps.test-rerun-1.outputs.status != 'success'
        id: test-rerun-2
        continue-on-error: true
        run: |
          echo ::set-output name=status::failure
          docker exec app.catroweb bin/behat -s ${{ matrix.testSuite }} --rerun
          echo ::set-output name=status::success

      # Rerun #3
      - name: 3. Rerun for Behat ${{ matrix.testSuite }} tests
        if: steps.test-run.outputs.status != 'success' && steps.test-rerun-1.outputs.status != 'success' && steps.test-rerun-2.outputs.status != 'success'
        id: test-rerun-3
        run: |
          docker exec app.catroweb bin/behat -f pretty -s ${{ matrix.testSuite }} --rerun

      - name: DEBUG
        if: always()
        run: |
          docker ps -a
          echo "--- App ---"
          docker logs app.catroweb
          echo "--- DB ---"
          docker logs db.catroweb.test
          echo "--- Chrome ---"
          docker logs chrome.catroweb

      - uses: actions/upload-artifact@v2.0.1
        if: failure()
        with:
          name: screenshots_${{ matrix.testSuite }}
          path: tests/testreports/screens
